# This is a sample Python script.
import os
from functools import reduce

import gradio as gr
from llama_cpp import Llama

# Press Shift+F10 to execute it or replace it with your code.
# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.

llama: Llama = None


def buddy(message, history, system, max_tokens, temperature, top_k, top_p, repeat_penalty, frequency_penalty):
    def deal_system(last, item):
        return last + "User: " + item[0] + "\n" + "Assistant: " + item[1]

    prompt = reduce(deal_system, history, system + "\n")
    prompt += "\nUser: " + message + "\n Assistant: "
    if llama is None:
        yield "è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
    else:
        answer = ""
        for i in llama(prompt, max_tokens=max_tokens, temperature=temperature, top_k=top_k, top_p=top_p,
                       repeat_penalty=repeat_penalty, frequency_penalty=frequency_penalty, stream=True):
            answer += i['choices'][0]['text']
            yield answer


def update_click():
    files = os.listdir("models/")
    model_files = []
    for file in files:
        if file.endswith(".gguf"):
            model_files.append(file)
    return gr.Dropdown(label="æ¨¡å‹", choices=model_files, scale=8)


def load_click(model_name, n_batch, n_thread, n_gpu_layers, n_ctx, progress=gr.Progress()):
    global llama
    progress(0)
    llama = Llama(model_path="models/" + model_name, n_ctx=n_ctx, n_threads=n_thread, n_batch=n_batch,
                  n_gpu_layers=n_gpu_layers)


def offical_load():
    return gr.TextArea(label="ç³»ç»Ÿæç¤ºè¯", lines=6, value="""You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.
Always answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
You like to use emojis. You can speak fluently in many languages, for example: English, Chinese.
You cannot access the internet, but you have vast knowledge, cutoff: 2021-09.
You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.

User: Hi.
Assistant: Hi, I'm Buddy, your AI assistant. How can I help you today?ğŸ˜Š""")


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    with gr.Blocks() as main:
        gr.Markdown("""# OpenBuddy WebUI""")
        with gr.Row() as row1:
            models = update_click()
            update_button = gr.Button(value="ğŸ”")
        with gr.Accordion("åŠ è½½è®¾ç½®", open=False) as tab1:
            n_batch = gr.Slider(label="n_batch", minimum=16, maximum=2048, value=512)
            n_thread = gr.Slider(label="çº¿ç¨‹æ•°", minimum=1, maximum=128, value=16)
            n_gpu_layers = gr.Slider(label="GPU å±‚æ•°", minimum=0, maximum=1024, value=0)
            n_ctx = gr.Slider(label="ä¸Šä¸‹æ–‡é•¿åº¦", minimum=2048, maximum=16384, value=4096)
        load_button = gr.Button(value="åŠ è½½")
        with gr.Accordion("é«˜çº§è®¾ç½®", open=False) as tab2:
            gr.Markdown("""## è¯´æ˜
ç³»ç»Ÿæç¤ºè¯ï¼ˆsystem promptï¼‰ç”¨äºæè¿°å¯¹è¯èƒŒæ™¯

æœ€å¤§ç”Ÿæˆè®°å·æ•°ï¼ˆmax tokensï¼‰ç”¨äºé™åˆ¶æ¨¡å‹æœ€å¤§çš„è¾“å‡ºé•¿åº¦

æ¸©åº¦ï¼ˆtemperatureï¼‰ç”¨æ¥ç•Œå®šéšæœºæ€§ï¼Œæ¸©åº¦è¶Šé«˜éšæœºæ€§è¶Šå¼º

top P ç”¨äºç­›é€‰å‡ºæ¦‚ç‡è¾ƒå¤§çš„å‰ PÃ—100% çš„å¯èƒ½ç»“æœ

top K æ˜¯æ¯æ¬¡åªè€ƒè™‘å‰ K ä¸ªå•è¯

é¢‘ç‡æƒ©ç½šï¼ˆfrequency penaltyï¼‰è¶Šæ¥è¿‘1ï¼Œä½¿ç”¨çš„è¯æ±‡è¶Šå¸¸è§ï¼Œè¶Šæ¥è¿‘-1ï¼Œä½¿ç”¨çš„è¯æ±‡è¶Šä¸å¸¸è§

é‡å¤æƒ©ç½šï¼ˆrepeat penaltyï¼‰è¶Šæ¥è¿‘1ï¼Œè¶Šåå¥½äºä½¿ç”¨å’Œå‰æ–‡ä¸é‡å¤çš„è¯æ±‡ï¼Œè¶Šæ¥è¿‘-1ï¼Œè¶Šåå¥½äºä½¿ç”¨å’Œå‰æ–‡é‡å¤çš„è¯æ±‡""")
            system_prompt = gr.TextArea(label="ç³»ç»Ÿæç¤ºè¯", lines=6)
            offical = gr.Button(value="å¯¼å…¥å®˜æ–¹æç¤ºè¯")
            max_tokens = gr.Slider(label="æœ€å¤§ç”Ÿæˆè®°å·æ•°", value=2048, minimum=128, maximum=4096, step=16)
            temperature = gr.Slider(label="æ¸©åº¦", minimum=0, maximum=1, step=0.01, value=1)
            top_p = gr.Slider(label="top P", minimum=0, maximum=1, value=0.9, step=0.01)
            top_k = gr.Slider(label="top K", minimum=0, maximum=1024, value=50, step=1)
            frequency_penalty = gr.Slider(label="é¢‘ç‡æƒ©ç½š", minimum=-1, maximum=1, step=0.01, value=1)
            repeat_penalty = gr.Slider(label="é‡å¤æƒ©ç½š", minimum=-1, maximum=1, step=0.01, value=1)
            offical.click(offical_load, outputs=[system_prompt])

        with gr.Tab("å¯¹è¯") as tab1:
            gr.ChatInterface(fn=buddy, additional_inputs=[system_prompt, max_tokens, temperature, top_k, top_p,
                                                          repeat_penalty, frequency_penalty])
            update_button.click(update_click, outputs=[models])
        load_button.click(load_click, inputs=[models, n_batch, n_thread, n_gpu_layers, n_ctx])

    main.queue().launch()

# See PyCharm help at https://www.jetbrains.com/help/pycharm/
